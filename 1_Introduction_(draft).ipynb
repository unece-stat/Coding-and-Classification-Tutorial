{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Introduction (draft)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU8SEesWldTp",
        "colab_type": "text"
      },
      "source": [
        "# 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1cAEj0ilghK",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "## 1.1 Coding and classification in statistical organisations \n",
        "--- \n",
        "Classifying a text description into pre-defined category is a very common task in statistical organization. In Labour Force Survey (LFS) survey, for example, a respondent is asked to describe their occupation in a  text which later classified as a statistical classification such as Standard Occupational Classification (SOC). This task, henceforth called coding & classification (C&C), is not limited to responses from survey questionnaire. Administrative register also requires classification of texts into codes. For example, a new company is asked to provide a description of their business activity for a business registration which is then classified into a statstistical classification such as Standard Industrial Classification (SIC). The categories that are classified at this stage of the statistical production process are used for all subsequent downstream tasks such as aggregation editing or imputation, therefore the quality of C&C is critical to ensure the quality of the final output.\n",
        "\n",
        "Traditionally, C&C was often done by human coders, experts who are trained to read a text description and classify it into pre-defined category. This manual classification is highly resource-intensive in terms of both time and money. For example, US Bureau of Labour Statistics (BLS) collects approximately 300,000 for its Survey of Occupational Injuries and Illnesses (SOII) and this require estimated 25,000 hours of manual work per year [ref_BLS].\n",
        "\n",
        "Statistical organisations are facing ever increasing demand to produce statistics at a faster pace with diminishing resources. Also, for analysis of big data from new data sources (e.g. twitter or web-scrapping), preparing and training human coders to process such huge amount of data are almost impossible. Machine learning (ML)-based text classification can offer great potential to automatise C&C to produce statistics more efficiently while maintaining the quality of official statistics.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9E9nUuBlhVa",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Text classification using machine learning\n",
        "---\n",
        "\n",
        "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions [ref_ML_wiki], relying on patterns derived from data. ML can be applied to text classification by programming computers to understand and analyse data in human language (also called \"natural language\"). ML-based text classification is used in many areas such as spam detection, sentiment analysis, topic labelling and so on.\n",
        "\n",
        "But what does it mean that machines “understand” human languages? Human language is highly complex, it is a result of thousands years of evolution with numerous rules as well as exceptions and irregularities. It is true that machines might not (yet) be able to understand a text with its nuances and contexts as humans do. However, for a limited scope of task, machines can work surprisingly well if they are given enough data to figure out the pattern.\n",
        "\n",
        "For example, let’s say, we have a text describing circumstance of work-related injury of someone as below:  \n",
        "\n",
        "<p align='center'>\"I cut my fingers while chopping vegetables”  </p> \n",
        "\n",
        "Humans can guess this person is more likely to be a cook than a statistician because we know that, <i> <b> from our life-long experiences, </i> </b> “chopping vegetables” is related to the work of a cook rather than a statistician. Machines do not have these experiences, but if they are given a lot of data that describes work-related injuries of a cook and a statistician, they can <i> <b> learn from the data </i> </b> which words are more associated with which job. Once machines figure out the pattern, they can predict when a new text (e.g. \"I sprained my waist while carrying frozen meat\") is given, whether this person is a cook or a statistician.\n",
        "\n",
        "Text classification shares commonality with traditional statistical methods in a sense that we want a (machine) model to figure out patterns from a data which can be used for the estimation or prediction. However, there are two differences due to the nature of input data (i.e. text written in human language).\n",
        "\n",
        "Firstly, <b> input data needs to be converted to numbers. </b> Most of computational models require input data to be in a numerical form. Then how a text description (e.g. \"my factory produces Acaia olive oil\") in input dataset can be converted to a set of numbers that maintain meaningful association with output category (e.g. industry category \"Manufacturing\")? One of popular approaches is to break down all text descriptions into smaller units to create a \"Bag of Words\" and encode each word into a number. Then, a model can calculate which word is more associated with which output category (e.g. a word \"factory\" tends to be more associated with category \"Manufacturing\" than category \"Accommodation\"). One complication is that text description can contain a lot of noise or low-information words. For example, possessive noun \"my\" may not be so important for classifying the text into an inudstry activity, hence can be removed to ensure that the size of the bag (in \"Bag of Words\") does not become too large. \n",
        "\n",
        "Secondly, <b> the relationship between input data and output category is highly complex. </b> Simple linear relation (e.g. the higher the X value, the higher the Y value) does not apply. While \"factory\" can have some level of association with category \"Manufacturing\", it could appear  commonly for other categories. For co-occurence of certain words can change how a text should be classified (e.g. \"I deliver foods from factory\" should be classified as category \"Trucking & Courier Services\" than cateogry \"Maunfacturing\"). The dimension of input data can be easily over few thousands (imagine range of vocabulary humans can use to describe their occupation). Therefore, ML models that are more to handle high-dimensional complext relationship are needed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NUy5j4mls_-",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 About this tutorial\n",
        "\n",
        "---\n",
        "\n",
        "The purpose of this tutorial is to provide an overview of the general process for the text classification. It  does not intend to give exhaustive list of text classification techniques nor comprehensive methodological details, but aims to demonstrate, as a starting point, how text classification can be done using few commonly used data preparation techniques and ML models.\n",
        "\n",
        "The rest of this tutorial consists as below that groups the steps needed for text classifcation into two broad stages.\n",
        "\n",
        "* <b> Data preparation</b>: in this stage, dataset is preprocessed to remove noise and unncessary inflections. Data in the textual form are converted to numbers so that ML models can process the data. Visualisation tools can be used to provide better understanding of the data which can lead to a new insight.\n",
        "\n",
        "* <b> ML modeling</b>: in this stage, dataset (in numeric form) is divided into training set and testing set. ML models are built using a training set, accuracy measures for each ML model are obtained using testing set to select the final model. This set of steps is relevant not only for text classification but in almost all ML application areas.\n",
        "\n",
        "Note that while many work flows in the industry start from setting a problem statement or obtaining data (e.g. \"Gather Data\" step from Google Developer Tool [ref_Google]), for statistical organisations, the coding and classification (C&C) task starts after data is already collected in previous phase (e.g. from survey, administrative registry, web-scrapping) [ref_GSBPM]. Therefore, it is assumed in this tutorial that dataset is already ready to be consumed for C&C. \n",
        "\n",
        "To demonstrate how the text classification can done in practice, programming codes (in python) are provided using real world data that are commonly found in business of statistics organisations. \n",
        "\n",
        "This tutorial was developed as a part of the United Nations Economic Commission for Europe (UNECE) High-Level Group for Modernisation of Official Statistics (HLG-MOS) Machine Learning Project. The project investigated the use of ML in statistical organisations for three areas: coding and classification, editing and imputation, and imagery, explored the quality aspects when using ML and ways how to integrate ML in statistical organisations. Readers who are interested in learning more are referred to the final report of the project [ref_MLProject]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4BVYJBuv2B8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## References\n",
        "---\n",
        "\n",
        "[ref_BLS] (not public yet) BLS ML pilot study report \n",
        "\n",
        "[ref_ML_wiki] <a href=\"https://en.wikipedia.org/wiki/Machine_learning\">Wikipedia</a>\n",
        "\n",
        "[ref_Google] <a href = \"https://developers.google.com/machine-learning/guides/text-classification/step-1\"> Google Developers Machine Learning Guide </a>\n",
        "\n",
        "[ref_GSBPM] <a href = \"https://statswiki.unece.org/display/GSBPM/Clickable+GSBPM+v5.1\"> Generic Statisitcal Business Process Model </a>\n",
        "\n",
        "[ref_MLProject] (not public yet) ML project wiki space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UczXwf1Ll6OK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Parking lot\n",
        "\n",
        "\n",
        "\n",
        "! Check how to use internal links in notebook: https://jupyter.brynmawr.edu/services/public/dblank/Jupyter%20Notebook%20Users%20Manual.ipynb#4.3.5-Notebook-Internal-Links\n",
        "\n",
        "To remove [ref_StatPoland] (not public yet) Statistics Poland pilot study report"
      ]
    }
  ]
}
