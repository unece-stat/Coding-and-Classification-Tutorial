{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Introduction (draft)",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU8SEesWldTp",
        "colab_type": "text"
      },
      "source": [
        "# 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1cAEj0ilghK",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "## 1.1 Coding and classification in statistical organisations \n",
        "--- \n",
        "Classifying a written description into pre-defined category is a very common task in statistical organisation. In Labour Force Survey (LFS) survey, for example, a respondent is asked to describe their job in a written text which later classified as a statistical classification such as Standard Occupational Classification (SOC). This task, henceforth called coding & classification (C&C), is not limited to responses from survey questionnaire. Administrative register also requires classification of a text into codes. For example, a new company is asked to provide a description of their business activity for for business registration which is then classified into Standard Industrial Classification (SIC). The categories that are classified at this stage of the statistical production process are used for all subsequent downstream tasks such as aggregation editing or imputation, therefore the quality of C&C is critical to ensure the quality of the final output.\n",
        "\n",
        "Traditionally, C&C was often done by human coders, experts who are trained to read a text description and classify it into pre-defined category. This manual classification is highly resource-intensive in terms of both time and money. For example, US Bureau of Labour Statistics (BLS) collects approximately 300,000 for its Survey of Occupational Injuries and Illnesses (SOII) which require estimated 25,000 hours of manual work per year [ref_BLS].\n",
        "\n",
        "Statistical organisations are facing ever increasing demand to produce statistics at a faster pace with diminishing resources. Also, for analysis of big data from new data sources (e.g. twitter or web-scrapping), preparing and training human coders to process such huge amount of data are almost impossible. \n",
        "\n",
        "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions [ref_ML_wiki], relying on patterns derived from data. ML can offer great potential to automatise C&C to produce statistics more efficiently while maintaining the quality of official statistics.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9E9nUuBlhVa",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Text classification using machine learning\n",
        "---\n",
        "\n",
        "Text classification is an area of ML application that concerns with programming computers to understand and analyse data in human language (also called \"natural language\"). It is used in many areas such as spam detection, sentiment analysis and so on.\n",
        "\n",
        "But what does it mean that machines “understand” human languages? Human language is highly complex, it is a result of hundreds or thousands years of evolution with numerous rules as well as exceptions and irregularities. Machines might not (yet) be able to understand a text with its nuances and contexts as humans do. However, for a limited scope of task, machines can work surprisingly well if they are given enough data to figure out the pattern.\n",
        "\n",
        "For example, let’s say, we have a text describing circumstance of work-related injury of someone as below:  \n",
        "\n",
        "<p align='center'>\"I cut my fingers while chopping vegetables”  </p> \n",
        "\n",
        "Humans can guess this person is more likely to be a cook than a statistician because we know that, <i> <b> from life-long experiences, </i> </b> “chopping vegetables” is related to the work of a cook rather than a statistician. Machines do not have these experiences, but if they are given a lot of data that describes work-related injuries of a cook and a statistician, they can <i> <b> learn from the data </i> </b> which words are more associated with which job. Once machines figure out the pattern, they can decide wheather a new text (e.g. \"I sprained my waist while carrying frozen meat\") is for a cook or a statistician.\n",
        "\n",
        "Text classification shares commonality with traditional statistical methods in a sense that we want a (machine) model to figure out patterns from a data which can be used for the estimation or prediction. However, there are differences due to the nature of input data (i.e. text written in human language).\n",
        "\n",
        "Firstly, <b> input data needs various processing to clean and convert it numbers. </b> Most of computational models require input data to be in a numerical form. Then how a text description (e.g. \"my factory produces Acaia olive oil\") in input dataset can be converted to a set of numbers that maintain meaningful association with output category (e.g. industry category \"Manufacturing\")? One of popular approaches is to break down all text descriptions into smaller units to create a \"Bag of Words\" and encode each word into a number. Then, a model can calculate which word is more associated with which output category (e.g. a word \"factory\" tends to be more associated with category \"Manufacturing\" than category \"Accommodation\"). Another complication is that text description can contain a lot of noise or low-information words. For example, \"Acaia\" (brand of olive oil) may not be so important for classifying the text into an inudstry activity, hence can be removed to ensure the size of the \"Bag of Words\" not too large. Texts that statistical organisations work with is generally less messy than social media data, but some level of text preprocessing is needed to clean the input data. \n",
        "\n",
        "Secondly, <b> the relationship between input data and output category is highly complex. </b> Simple linear relation (e.g. the higher the X value, the higher the Y value) does not apply. While \"factory\" can have some level of association with category \"Manufacturing\", it could appear  commonly for other categories. For co-occurence of certain words can change how a text should be classified (e.g. \"I deliver foods from factory\" should be classified as category \"Trucking & Courier Services\" than cateogry \"Maunfacturing\"). The dimension of input data can be easily over few thousands (imagine range of vocabulary humans can use to describe their occupation). Therefore, ML models that are more to handle high-dimensional complext relationship are needed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NUy5j4mls_-",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 About this tutorial\n",
        "\n",
        "---\n",
        "\n",
        "The purpose of this tutorial is to provide an overview of the general process for the text classification. It  does not intend to give exhaustive list of text classification techniques nor comprehensive methodological details, but aims to demonstrate as a starting point how text classification can be done using few commonly used data preparation techniques and ML models.\n",
        "\n",
        "The rest of this tutorial consists as below that groups the steps needed for text classifcation into two broad stages.\n",
        "\n",
        "* <b> Data preparation</b>: in this stage, dataset is preprocessed to remove noise and unncessary inflections. Data in the textual form are converted to numbers so that ML models can process the data. Visualisation tools can be used to provide better understanding of the data which can lead to a new insight.\n",
        "\n",
        "* <b> ML modeling</b>: in this stage, dataset (in numeric form) is divided into training set and testing set. ML models are built using training set, accuracy measures for each ML model are obtained using testing set to select the final model. This set of steps is relevant not only for text classification but in almost all ML application areas.\n",
        "\n",
        "Note that while many work flows in the industry start from obtaining data (e.g. \"Gather Data\" step from Google Developer Tool [ref_Google], \"Get data\" step from Microsoft Azure [ref_MS]), for statistical organisations, the coding and classification (C&C) task usually starts after data is already collected in previous phase (e.g. from survey, administrative registry, web-scrapping) [ref_GSBPM]. Therefore, it is assumed in this tutorial that dataset is already ready to be consumed for C&C. \n",
        "\n",
        "To demonstrate how the text classification can done in practice, programming scripts (in python) are provided. Real world data (ECOICOP data from Statistiscs Poland [ref_StatPoland]) that are commonly found in daily business of statistics organisations are used as a working example. \n",
        "\n",
        "This tutorial was developed as a part of the United Nations Economic Commission for Europe (UNECE) High-Level Group for Modernisation of Official Statistics (HLG-MOS) Machine Learning Project. The project investigated the use of ML in statistical organisations for three areas: coding and classification, editing and imputation, and imagery. Readers who are interested in more ML application examples, quality issues related to ML and organisational considerations regarding integration of ML in production process are referred to the final report of the project [ref_MLProject].  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4BVYJBuv2B8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## References\n",
        "---\n",
        "\n",
        "! Check how to use internal links in notebook: https://jupyter.brynmawr.edu/services/public/dblank/Jupyter%20Notebook%20Users%20Manual.ipynb#4.3.5-Notebook-Internal-Links\n",
        "\n",
        "[ref_BLS] (not public yet) BLS ML pilot study report \n",
        "\n",
        "[ref_ML_wiki] <a href=\"https://en.wikipedia.org/wiki/Machine_learning\">Wikipedia</a>\n",
        "\n",
        "[ref_Google] <a href = \"https://developers.google.com/machine-learning/guides/text-classification/step-1\"> Google Developers Machine Learning Guide </a>\n",
        "\n",
        "[ref_MS] ms azure\n",
        "\n",
        "[ref_GSBPM] <a href = \"https://statswiki.unece.org/display/GSBPM/Clickable+GSBPM+v5.1\"> Generic Statisitcal Business Process Model </a>\n",
        "\n",
        "[ref_StatPoland] (not public yet) Statistics Poland pilot study report\n"
      ]
    }
  ]
}